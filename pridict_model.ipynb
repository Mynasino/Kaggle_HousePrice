{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.chdir('/home/mynasino/kaggle_houseprice/model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MiscVal_train_X = pd.read_csv('data_process/MiscVal_data.csv').drop(['Unnamed: 0'],axis=1).as_matrix()[0:1460]\n",
    "MiscVal_pridict_X = pd.read_csv('data_process/MiscVal_data.csv').drop(['Unnamed: 0'],axis=1).as_matrix()[1460:2919]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_df = pd.read_csv('data_process/one_hot_features.csv').drop(['Unnamed: 0'], axis=1)\n",
    "numerical_df = pd.read_csv('data_process/numerical_with_pridict.csv').drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)\n",
    "\n",
    "df = pd.concat([one_hot_df,numerical_df],axis=1)\n",
    "df_y = pd.read_csv('/home/mynasino/kaggle_houseprice/data/train.csv')['SalePrice']\n",
    "\n",
    "train_X = df.as_matrix()[0:1460]\n",
    "train_y = df_y.as_matrix().reshape([1460,1])\n",
    "pridict_X = df.as_matrix()[1460:2919]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Xavier_init(fan_in,fan_out,constant=1):\n",
    "    low = -constant * np.sqrt(6/(fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in,fan_out),minval=low,maxval=high,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "batch_size = 16\n",
    "num_examples = 1460\n",
    "\n",
    "def data_iter():\n",
    "    idx = list(range(num_examples))\n",
    "    random.shuffle(idx)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = np.array(idx[i:min(i + batch_size, num_examples)]) #np.array() missed\n",
    "        yield np.take(train_X, j, axis=0), np.take(MiscVal_train_X, j).reshape([len(j),1]), np.take(train_y, j).reshape([len(j),1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_units = 377\n",
    "h1_units = 50\n",
    "h2_units = 8\n",
    "\n",
    "W1 = tf.Variable(Xavier_init(in_units,h1_units))\n",
    "b1 = tf.Variable(tf.constant(0.1,shape=[h1_units]))\n",
    "W2 = tf.Variable(Xavier_init(h1_units,h2_units))\n",
    "b2 = tf.Variable(tf.constant(0.1,shape=[h2_units]))\n",
    "W3 = tf.Variable(Xavier_init(h2_units,1))\n",
    "b3 = tf.Variable(tf.constant(0.1,shape=[1]))\n",
    "\n",
    "in_X = tf.placeholder(tf.float32,[None,in_units])\n",
    "Misc_X = tf.placeholder(tf.float32,[None,1])\n",
    "hidden1 = tf.nn.relu(tf.matmul(in_X,W1) + b1)\n",
    "hidden2 = tf.nn.relu(tf.matmul(hidden1,W2) + b2)\n",
    "y = tf.nn.relu(tf.matmul(hidden2,W3) + b3) + Misc_X\n",
    "\n",
    "y_ = tf.placeholder(tf.float32,[None,1])\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.log(y + 1) - tf.log(y_ + 1))))\n",
    "loss_with_L2 = loss + 0.1 * tf.reduce_mean(tf.square(W1))\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss_with_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train_loss 5.42773 test_loss 5.41646\n",
      "epoch 1 train_loss 4.02322 test_loss 4.00996\n",
      "epoch 2 train_loss 3.18443 test_loss 3.17021\n",
      "epoch 3 train_loss 2.59137 test_loss 2.57626\n",
      "epoch 4 train_loss 2.13372 test_loss 2.11781\n",
      "epoch 5 train_loss 1.76121 test_loss 1.74433\n",
      "epoch 6 train_loss 1.44781 test_loss 1.42978\n",
      "epoch 7 train_loss 1.17959 test_loss 1.16023\n",
      "epoch 8 train_loss 0.947999 test_loss 0.927088\n",
      "epoch 9 train_loss 0.750065 test_loss 0.727381\n",
      "epoch 10 train_loss 0.584813 test_loss 0.560126\n",
      "epoch 11 train_loss 0.458322 test_loss 0.431949\n",
      "epoch 12 train_loss 0.3715 test_loss 0.344763\n",
      "epoch 13 train_loss 0.318886 test_loss 0.293828\n",
      "epoch 14 train_loss 0.280301 test_loss 0.25881\n",
      "epoch 15 train_loss 0.252065 test_loss 0.234066\n",
      "epoch 16 train_loss 0.229015 test_loss 0.21373\n",
      "epoch 17 train_loss 0.209635 test_loss 0.196783\n",
      "epoch 18 train_loss 0.195248 test_loss 0.183729\n",
      "epoch 19 train_loss 0.185309 test_loss 0.174892\n",
      "epoch 20 train_loss 0.178082 test_loss 0.16835\n",
      "epoch 21 train_loss 0.172485 test_loss 0.163616\n",
      "epoch 22 train_loss 0.168586 test_loss 0.160167\n",
      "epoch 23 train_loss 0.165918 test_loss 0.157231\n",
      "epoch 24 train_loss 0.162901 test_loss 0.155053\n",
      "epoch 25 train_loss 0.159766 test_loss 0.152893\n",
      "epoch 26 train_loss 0.157681 test_loss 0.150922\n",
      "epoch 27 train_loss 0.155621 test_loss 0.149231\n",
      "epoch 28 train_loss 0.153665 test_loss 0.147678\n",
      "epoch 29 train_loss 0.152348 test_loss 0.146161\n",
      "epoch 30 train_loss 0.150641 test_loss 0.144881\n",
      "epoch 31 train_loss 0.149011 test_loss 0.144103\n",
      "epoch 32 train_loss 0.147815 test_loss 0.142482\n",
      "epoch 33 train_loss 0.147223 test_loss 0.141351\n",
      "epoch 34 train_loss 0.145497 test_loss 0.140344\n",
      "epoch 35 train_loss 0.144422 test_loss 0.139249\n",
      "epoch 36 train_loss 0.14323 test_loss 0.13838\n",
      "epoch 37 train_loss 0.142454 test_loss 0.137218\n",
      "epoch 38 train_loss 0.141141 test_loss 0.13633\n",
      "epoch 39 train_loss 0.140303 test_loss 0.135337\n",
      "epoch 40 train_loss 0.139489 test_loss 0.134618\n",
      "epoch 41 train_loss 0.138199 test_loss 0.133938\n",
      "epoch 42 train_loss 0.137434 test_loss 0.133151\n",
      "epoch 43 train_loss 0.136645 test_loss 0.132665\n",
      "epoch 44 train_loss 0.136182 test_loss 0.131683\n",
      "epoch 45 train_loss 0.135108 test_loss 0.13131\n",
      "epoch 46 train_loss 0.134158 test_loss 0.130086\n",
      "epoch 47 train_loss 0.133522 test_loss 0.129546\n",
      "epoch 48 train_loss 0.132954 test_loss 0.128569\n",
      "epoch 49 train_loss 0.13242 test_loss 0.127918\n",
      "epoch 50 train_loss 0.1319 test_loss 0.127435\n",
      "epoch 51 train_loss 0.131566 test_loss 0.12761\n",
      "epoch 52 train_loss 0.1313 test_loss 0.12765\n",
      "epoch 53 train_loss 0.130185 test_loss 0.125646\n",
      "epoch 54 train_loss 0.129827 test_loss 0.125017\n",
      "epoch 55 train_loss 0.129422 test_loss 0.125216\n",
      "epoch 56 train_loss 0.128783 test_loss 0.124518\n",
      "epoch 57 train_loss 0.128437 test_loss 0.123962\n",
      "epoch 58 train_loss 0.127938 test_loss 0.123661\n",
      "epoch 59 train_loss 0.12721 test_loss 0.122566\n",
      "epoch 60 train_loss 0.127881 test_loss 0.122378\n",
      "epoch 61 train_loss 0.12761 test_loss 0.122297\n",
      "epoch 62 train_loss 0.126061 test_loss 0.121585\n",
      "epoch 63 train_loss 0.126654 test_loss 0.121339\n",
      "epoch 64 train_loss 0.12581 test_loss 0.12159\n",
      "epoch 65 train_loss 0.124839 test_loss 0.120264\n",
      "epoch 66 train_loss 0.124499 test_loss 0.120336\n",
      "epoch 67 train_loss 0.124259 test_loss 0.119808\n",
      "epoch 68 train_loss 0.123859 test_loss 0.119265\n",
      "epoch 69 train_loss 0.124355 test_loss 0.12023\n",
      "epoch 70 train_loss 0.123802 test_loss 0.119637\n",
      "epoch 71 train_loss 0.123056 test_loss 0.118804\n",
      "epoch 72 train_loss 0.12274 test_loss 0.118308\n",
      "epoch 73 train_loss 0.122472 test_loss 0.118188\n",
      "epoch 74 train_loss 0.122215 test_loss 0.117821\n",
      "epoch 75 train_loss 0.122867 test_loss 0.118512\n",
      "epoch 76 train_loss 0.121632 test_loss 0.117647\n",
      "epoch 77 train_loss 0.12162 test_loss 0.117519\n",
      "epoch 78 train_loss 0.121127 test_loss 0.117076\n",
      "epoch 79 train_loss 0.121479 test_loss 0.117783\n",
      "epoch 80 train_loss 0.120654 test_loss 0.116662\n",
      "epoch 81 train_loss 0.120292 test_loss 0.116504\n",
      "epoch 82 train_loss 0.121028 test_loss 0.116257\n",
      "epoch 83 train_loss 0.120122 test_loss 0.11574\n",
      "epoch 84 train_loss 0.119769 test_loss 0.115556\n",
      "epoch 85 train_loss 0.120083 test_loss 0.115928\n",
      "epoch 86 train_loss 0.120258 test_loss 0.115782\n",
      "epoch 87 train_loss 0.119687 test_loss 0.115124\n",
      "epoch 88 train_loss 0.119117 test_loss 0.114951\n",
      "epoch 89 train_loss 0.119119 test_loss 0.114622\n",
      "epoch 90 train_loss 0.119116 test_loss 0.114535\n",
      "epoch 91 train_loss 0.11873 test_loss 0.114235\n",
      "epoch 92 train_loss 0.12029 test_loss 0.115191\n",
      "epoch 93 train_loss 0.118759 test_loss 0.114139\n",
      "epoch 94 train_loss 0.119245 test_loss 0.114671\n",
      "epoch 95 train_loss 0.118667 test_loss 0.114295\n",
      "epoch 96 train_loss 0.118681 test_loss 0.114009\n",
      "epoch 97 train_loss 0.117749 test_loss 0.113478\n",
      "epoch 98 train_loss 0.117897 test_loss 0.113849\n",
      "epoch 99 train_loss 0.117984 test_loss 0.113931\n",
      "epoch 100 train_loss 0.119656 test_loss 0.115003\n",
      "epoch 101 train_loss 0.117135 test_loss 0.112701\n",
      "epoch 102 train_loss 0.117635 test_loss 0.113905\n",
      "epoch 103 train_loss 0.117728 test_loss 0.113485\n",
      "epoch 104 train_loss 0.117612 test_loss 0.113244\n",
      "epoch 105 train_loss 0.116249 test_loss 0.112558\n",
      "epoch 106 train_loss 0.116067 test_loss 0.112416\n",
      "epoch 107 train_loss 0.115999 test_loss 0.112378\n",
      "epoch 108 train_loss 0.115519 test_loss 0.111941\n",
      "epoch 109 train_loss 0.116608 test_loss 0.11338\n",
      "epoch 110 train_loss 0.115356 test_loss 0.11231\n",
      "epoch 111 train_loss 0.115237 test_loss 0.111805\n",
      "epoch 112 train_loss 0.115064 test_loss 0.111098\n",
      "epoch 113 train_loss 0.115324 test_loss 0.112058\n",
      "epoch 114 train_loss 0.114618 test_loss 0.111532\n",
      "epoch 115 train_loss 0.115601 test_loss 0.112704\n",
      "epoch 116 train_loss 0.114976 test_loss 0.111509\n",
      "epoch 117 train_loss 0.113963 test_loss 0.110557\n",
      "epoch 118 train_loss 0.114038 test_loss 0.110811\n",
      "epoch 119 train_loss 0.113577 test_loss 0.110171\n",
      "epoch 120 train_loss 0.113558 test_loss 0.110271\n",
      "epoch 121 train_loss 0.113374 test_loss 0.110147\n",
      "epoch 122 train_loss 0.114626 test_loss 0.10995\n",
      "epoch 123 train_loss 0.114539 test_loss 0.111115\n",
      "epoch 124 train_loss 0.113653 test_loss 0.110284\n",
      "epoch 125 train_loss 0.113584 test_loss 0.110279\n",
      "epoch 126 train_loss 0.112752 test_loss 0.10967\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch_i in range(2000):\n",
    "    for batch_X, MiscVal_batch_X, batch_y in data_iter():\n",
    "        sess.run(train_step,feed_dict={in_X: batch_X, Misc_X:MiscVal_batch_X, y_: batch_y})\n",
    "    \n",
    "    #generate train_examples and test_examples\n",
    "    train_id = list(range(0, 730))\n",
    "    test_id = list(range(730,1460))\n",
    "    arr_train_x = np.take(train_X, train_id, axis=0)\n",
    "    arr_Misc_train_x = np.take(MiscVal_train_X, train_id).reshape([len(train_id),1])\n",
    "    arr_train_y = np.take(train_y, train_id).reshape([len(train_id),1])\n",
    "    arr_test_x = np.take(train_X, test_id, axis=0)\n",
    "    arr_Misc_test_x = np.take(MiscVal_train_X, test_id).reshape([len(test_id),1])\n",
    "    arr_test_y = np.take(train_y, test_id).reshape([len(test_id),1])\n",
    "        \n",
    "    #calculate train loss\n",
    "    train_loss.append(sess.run(loss,feed_dict={in_X: arr_train_x, Misc_X:arr_Misc_train_x ,y_: arr_train_y}))\n",
    "        \n",
    "    #calculate test loss\n",
    "    test_loss.append(sess.run(loss,feed_dict={in_X: arr_test_x, Misc_X:arr_Misc_test_x ,y_:arr_test_y}))\n",
    "        \n",
    "    print(\"epoch %d\" % epoch_i, \"train_loss\", train_loss[-1], \"test_loss\", test_loss[-1])\n",
    "    \n",
    "    #add early stop here to impove generalization ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fedac6667b8>,\n",
       " <matplotlib.lines.Line2D at 0x7fedac666a20>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fedc067b978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "s = 0\n",
    "e = 100\n",
    "\n",
    "plt.plot(range(s,e),train_loss[s:e],range(s,e),test_loss[s:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
